{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b322a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from collections import Counter\n",
    "from collections.abc import MutableMapping\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import community\n",
    "import seaborn as sns\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import time\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import advertools as adv\n",
    "\n",
    "#Logger\n",
    "logging.basicConfig(filename='Text-TweetsAnalysis.log', format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c604b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# PARAMETERS #\n",
    "##############\n",
    "\n",
    "client = MongoClient(username='XXX', password='XXX')\n",
    "\n",
    "DatabaseName = \"Streaming\"\n",
    "CollectionName = \"Campanya-Interactions\"\n",
    "\n",
    "db = client[DatabaseName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf65542",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#  STOPWORDS  #\n",
    "###############\n",
    "\n",
    "stopwords_spanish = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords_english = nltk.corpus.stopwords.words('english')\n",
    "stopwords_catalan = adv.stopwords['catalan']\n",
    "\n",
    "custom_stopwords = ['none', '', 'q', 'l', '\\n', 'rt']\n",
    "\n",
    "stopwords = stopwords_spanish + stopwords_english + list(stopwords_catalan)\n",
    "stopwords.extend(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# TWEETS TEXT ANALYSIS  #\n",
    "#########################\n",
    "\n",
    "#He ampliat l'emoji_pattern per a que tingui també en compte els nous emojis\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def clean_tweet(twit_text):\n",
    "    twit_text_filtered = re.sub(r'[#@]', '', twit_text)\n",
    "    twit_text_filtered_2 = re.sub(r'(\\s)http\\S+', '', twit_text_filtered)\n",
    "    twit_text_filtered_3 = emoji_pattern.sub(r'', twit_text_filtered_2)\n",
    "    #twit_text_filtered_3 = twit_text_filtered_2.encode('ascii', 'ignore').decode('ascii') \n",
    "    return twit_text_filtered_3\n",
    "\n",
    "\n",
    "def load_tweets(collection):\n",
    "    \"\"\"Extracts the ObjectID and created_at of users\n",
    "    \n",
    "    Keyword arguments:\n",
    "    user_collection -- MongoDB Users' Collection\n",
    "    \"\"\"\n",
    "    pipeline = [\n",
    "                {\n",
    "                    '$match': {\n",
    "                        'text': {            \n",
    "                            '$exists': True\n",
    "                        },\n",
    "                    }\n",
    "                }, {\n",
    "                    '$project': {\n",
    "                        '_id': True, \n",
    "                        'text': True\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    print(\"Query\", end=\" \")\n",
    "    tweets = collection.aggregate(pipeline, allowDiskUse=True)\n",
    "    print(\"OK; List\", end=\" \")\n",
    "    tweets = list(tweets)\n",
    "    print(\"OK; Total tweets:\", len(tweets))\n",
    "    return tweets\n",
    "\n",
    "\n",
    "tweets_list = load_tweets(db[CollectionName])\n",
    "\n",
    "text_total = \"\"\n",
    "for tweet in progress_bar(tweets_list):\n",
    "    text_total += \" \" + clean_tweet(str(tweet['text']))\n",
    "\n",
    "text_length = len(text_total)\n",
    "print(text_length)\n",
    "\n",
    "print(\"Divide string\")\n",
    "sub_string1 = text_total[0:int(text_length/4)]\n",
    "print(len(sub_string1))\n",
    "\n",
    "print(\"SPLIT\")\n",
    "tokens = [w.strip('“”.,;:-():!?-‘’|/•&+* ') for w in re.split(r\"[ ']+\", sub_string1.lower())]\n",
    "\n",
    "# Create counter\n",
    "counts = Counter(tokens)\n",
    "print(len(counts))\n",
    "counts.most_common(150)\n",
    "\n",
    "# TEMP\n",
    "important_tokens = [important_token for important_token in tokens if important_token not in stopwords]\n",
    "print(len(important_tokens))\n",
    "\n",
    "# Create counter\n",
    "counts = Counter(important_tokens)\n",
    "print(len(counts))\n",
    "counts.most_common(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# REMOVE STOPWORDS  #\n",
    "#####################\n",
    "\n",
    "stopwords_spanish = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords_english = nltk.corpus.stopwords.words('english')\n",
    "stopwords_catalan = adv.stopwords['catalan']\n",
    "\n",
    "custom_stopwords = ['none', '', 'q', 'l', '\\n', 'rt']\n",
    "\n",
    "stopwords = stopwords_spanish + stopwords_english + list(stopwords_catalan)\n",
    "stopwords.extend(custom_stopwords)\n",
    "\n",
    "important_tokens = [important_token for important_token in tokens if important_token not in stopwords]\n",
    "print(len(important_tokens))\n",
    "\n",
    "# Create counter\n",
    "counts = Counter(important_tokens)\n",
    "print(len(counts))\n",
    "counts.most_common(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TWEET TEXT ANALYSIS BY COMMUNITY IMPROVED (apostrophe)  #\n",
    "###########################################################\n",
    "\n",
    "# MADRID\n",
    "COMMUNITIES_LIST = [\"VOX\", \"PODEMOS\", \"PSOE\", \"PP\", \"MAS_MAD\", \"CS\"]\n",
    "\n",
    "# ANDALUSIA\n",
    "# COMMUNITIES_LIST = [\"VOX\", \"POR_AND\", \"PSOE\", \"PP\", \"ADELANTE_AND\", \"CS\"]\n",
    "\n",
    "def load_tweets_by_community(collection, community):\n",
    "    \"\"\"Extracts the ObjectID and created_at of users\n",
    "    \n",
    "    Keyword arguments:\n",
    "    user_collection -- MongoDB Users' Collection\n",
    "    \"\"\"\n",
    "    pipeline = [\n",
    "                {\n",
    "                    '$match': {\n",
    "                        'ACTIVE_community': community, \n",
    "                    }\n",
    "                }, {\n",
    "                    '$project': {\n",
    "                        '_id': True, \n",
    "                        'text': True\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    print(\"Query\", end=\" \")\n",
    "    tweets = collection.aggregate(pipeline, allowDiskUse=True)\n",
    "    print(\"OK; List\", end=\" \")\n",
    "    tweets = list(tweets)\n",
    "    print(\"OK; Total tweets:\", len(tweets))\n",
    "    return tweets\n",
    "\n",
    "for community in COMMUNITIES_LIST:\n",
    "    print(\"******************************************************\")\n",
    "    print(\"                      \" + community)\n",
    "    print(\"******************************************************\")\n",
    "    tweets_by_community_list = load_tweets_by_community(db[CollectionName], community)\n",
    "\n",
    "    text_total = \"\"\n",
    "    for tweet in tweets_by_community_list:\n",
    "        text_total += \" \" + clean_tweet(str(tweet['text']))\n",
    "\n",
    "    tokens = [w.strip('“”.,;:-():!?-‘’|/•&+* ') for w in re.split(r\"[ ']+\", text_total.lower())]   \n",
    "    important_tokens = [important_token for important_token in tokens if important_token not in stopwords]\n",
    "\n",
    "    counts = Counter(important_tokens)\n",
    "    print(\"Total Tokens: \" + str(len(counts)))\n",
    "    print(counts.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4770b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
